#!/usr/bin/env python3
"""
API-based video processing to replace MiniCPM
æ”¯æŒå¤šç§APIæä¾›å•†çš„å¤šæ¨¡æ€è§†é¢‘å¤„ç†
"""

import base64
import io
import json
import requests
from PIL import Image
from typing import List, Dict, Any, Optional
import os

class APIVideoProcessor:
    def __init__(self, provider="openai", api_key=None, base_url=None):
        """
        åˆå§‹åŒ–APIè§†é¢‘å¤„ç†å™¨
        
        Args:
            provider: APIæä¾›å•† ("openai", "claude", "gemini", "azure")
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.provider = provider
        self.base_url = base_url
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„api_keyï¼Œç„¶åå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œæœ€åä»ç¯å¢ƒå˜é‡è¯»å–
        if api_key:
            self.api_key = api_key
        else:
            # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
            try:
                from config_loader import get_openai_config
                openai_config = get_openai_config()
                if openai_config.get('api_key'):
                    self.api_key = openai_config['api_key']
                    if openai_config.get('base_url'):
                        self.base_url = openai_config['base_url']
                    print(f"âœ… ä»é…ç½®æ–‡ä»¶è¯»å–APIå¯†é’¥: {self.api_key[:10]}...")
                    print(f"âœ… ä»é…ç½®æ–‡ä»¶è¯»å–Base URL: {self.base_url}")
                else:
                    self.api_key = os.getenv(f"{provider.upper()}_API_KEY")
            except Exception as e:
                print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                self.api_key = os.getenv(f"{provider.upper()}_API_KEY")
        
        if not self.api_key:
            raise ValueError(f"API key not found for {provider}")
    
    def image_to_base64(self, image: Image.Image) -> str:
        """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64ç¼–ç """
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        return img_str
    
    def video_captioning(self, frames: List[Image.Image], character_refs: List[Dict], 
                        transcript: str, video_name: str) -> str:
        """
        è§†é¢‘å­—å¹•ç”Ÿæˆ - æ›¿ä»£MiniCPMçš„captionåŠŸèƒ½
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            character_refs: è§’è‰²å‚è€ƒä¿¡æ¯
            transcript: è¯­éŸ³è½¬å½•æ–‡æœ¬
            
        Returns:
            ç”Ÿæˆçš„è§†é¢‘æè¿°æ–‡æœ¬
        """
        if self.provider == "openai":
            return self._gpt4v_caption(frames, character_refs, transcript)
        elif self.provider == "claude":
            return self._claude_caption(frames, character_refs, transcript)
        elif self.provider == "gemini":
            return self._gemini_caption(frames, character_refs, transcript)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def video_frame_selection(self, frames: List[Image.Image], description: str, 
                            duration: float) -> int:
        """
        è§†é¢‘å¸§é€‰æ‹© - æ›¿ä»£MiniCPMçš„frame selectionåŠŸèƒ½
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            description: åœºæ™¯æè¿°
            duration: æ‰€éœ€æ—¶é•¿
            
        Returns:
            æœ€ä½³èµ·å§‹å¸§çš„ç´¢å¼•
        """
        if self.provider == "openai":
            return self._gpt4v_frame_selection(frames, description, duration)
        elif self.provider == "claude":
            return self._claude_frame_selection(frames, description, duration)
        elif self.provider == "gemini":
            return self._gemini_frame_selection(frames, description, duration)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def _gpt4v_caption(self, frames: List[Image.Image], character_refs: List[Dict], 
                      transcript: str) -> str:
        """ä½¿ç”¨GPT-4oè¿›è¡Œè§†é¢‘å­—å¹•ç”Ÿæˆ"""
        import openai
        
        # é…ç½®OpenAIå®¢æˆ·ç«¯
        if self.base_url:
            openai.api_base = self.base_url
            print(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰APIç«¯ç‚¹: {self.base_url}")
        
        # æ„å»ºè§’è‰²å‚è€ƒæ–‡æœ¬
        char_text = ""
        for char in character_refs:
            char_text += f"Character: {char['name']}\n"
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        You are analyzing a video segment. Below are character references and video frames.
        
        Character References:
        {char_text}
        
        Audio Transcript: {transcript}
        
        Please provide a detailed video scene description including:
        1. Characters' emotions and actions
        2. Motion dynamics and scene flow
        3. Visual elements and atmosphere
        4. Use character names if they appear in the video
        
        Respond in English only. Be descriptive and engaging.
        """
        
        # å‡†å¤‡æ¶ˆæ¯å†…å®¹
        content = [{"type": "text", "text": prompt}]
        
        # æ·»åŠ è§†é¢‘å¸§
        for frame in frames:
            img_b64 = self.image_to_base64(frame)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}
            })
        
        try:
            # ä½¿ç”¨æ–°çš„OpenAI APIæ ¼å¼
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            response = client.chat.completions.create(
                model="gpt-4o",  # ä½¿ç”¨GPT-4o
                messages=[{"role": "user", "content": content}],
                max_tokens=500
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"GPT-4o API error: {e}")
            return "Video analysis failed"
    
    def _gpt4v_frame_selection(self, frames: List[Image.Image], description: str, 
                              duration: float) -> int:
        """ä½¿ç”¨GPT-4oè¿›è¡Œè§†é¢‘å¸§é€‰æ‹©"""
        import openai
        
        # é…ç½®OpenAIå®¢æˆ·ç«¯
        if self.base_url:
            openai.api_base = self.base_url
            print(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰APIç«¯ç‚¹: {self.base_url}")
        
        prompt = f"""
        You are analyzing {len(frames)} consecutive video frames to find the best sequence.
        
        Scene Description: "{description}"
        Required Duration: {duration:.1f} seconds
        
        Requirements:
        1. Analyze ALL {len(frames)} frames
        2. Choose the best starting frame (0-{len(frames)-1})
        3. Maximum starting frame: {len(frames) - int(duration)}
        4. Return ONLY a single number - the frame index
        5. Select frames that best match the scene description
        
        Respond with just the frame number (0-{len(frames)-1}):
        """
        
        content = [{"type": "text", "text": prompt}]
        
        # æ·»åŠ è§†é¢‘å¸§
        for i, frame in enumerate(frames):
            img_b64 = self.image_to_base64(frame)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}
            })
        
        try:
            # ä½¿ç”¨æ–°çš„OpenAI APIæ ¼å¼
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            response = client.chat.completions.create(
                model="gpt-4o",  # ä½¿ç”¨GPT-4o
                messages=[{"role": "user", "content": content}],
                max_tokens=10
            )
            
            # æå–æ•°å­—
            import re
            result = response.choices[0].message.content.strip()
            numbers = re.findall(r'\d+', result)
            if numbers:
                frame_idx = int(numbers[0])
                return max(0, min(frame_idx, len(frames) - int(duration)))
            return 0
        except Exception as e:
            print(f"GPT-4o frame selection error: {e}")
            return 0
    
    def _claude_caption(self, frames: List[Image.Image], character_refs: List[Dict], 
                       transcript: str) -> str:
        """ä½¿ç”¨Claudeè¿›è¡Œè§†é¢‘å­—å¹•ç”Ÿæˆ"""
        import anthropic
        
        client = anthropic.Anthropic(api_key=self.api_key)
        
        # æ„å»ºè§’è‰²å‚è€ƒæ–‡æœ¬
        char_text = ""
        for char in character_refs:
            char_text += f"Character: {char['name']}\n"
        
        prompt = f"""
        You are analyzing a video segment. Below are character references and video frames.
        
        Character References:
        {char_text}
        
        Audio Transcript: {transcript}
        
        Please provide a detailed video scene description including:
        1. Characters' emotions and actions
        2. Motion dynamics and scene flow
        3. Visual elements and atmosphere
        4. Use character names if they appear in the video
        
        Respond in English only. Be descriptive and engaging.
        """
        
        # å‡†å¤‡å›¾åƒæ•°æ®
        images = []
        for frame in frames:
            img_b64 = self.image_to_base64(frame)
            images.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": img_b64
                }
            })
        
        try:
            response = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                messages=[{
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}] + images
                }]
            )
            return response.content[0].text.strip()
        except Exception as e:
            print(f"Claude API error: {e}")
            return "Video analysis failed"
    
    def _claude_frame_selection(self, frames: List[Image.Image], description: str, 
                               duration: float) -> int:
        """ä½¿ç”¨Claudeè¿›è¡Œè§†é¢‘å¸§é€‰æ‹©"""
        import anthropic
        
        client = anthropic.Anthropic(api_key=self.api_key)
        
        prompt = f"""
        You are analyzing {len(frames)} consecutive video frames to find the best sequence.
        
        Scene Description: "{description}"
        Required Duration: {duration:.1f} seconds
        
        Requirements:
        1. Analyze ALL {len(frames)} frames
        2. Choose the best starting frame (0-{len(frames)-1})
        3. Maximum starting frame: {len(frames) - int(duration)}
        4. Return ONLY a single number - the frame index
        5. Select frames that best match the scene description
        
        Respond with just the frame number (0-{len(frames)-1}):
        """
        
        # å‡†å¤‡å›¾åƒæ•°æ®
        images = []
        for frame in frames:
            img_b64 = self.image_to_base64(frame)
            images.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": img_b64
                }
            })
        
        try:
            response = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=10,
                messages=[{
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}] + images
                }]
            )
            
            # æå–æ•°å­—
            import re
            result = response.content[0].text.strip()
            numbers = re.findall(r'\d+', result)
            if numbers:
                frame_idx = int(numbers[0])
                return max(0, min(frame_idx, len(frames) - int(duration)))
            return 0
        except Exception as e:
            print(f"Claude frame selection error: {e}")
            return 0
    
    def _gemini_caption(self, frames: List[Image.Image], character_refs: List[Dict], 
                       transcript: str) -> str:
        """ä½¿ç”¨Geminiè¿›è¡Œè§†é¢‘å­—å¹•ç”Ÿæˆ"""
        import google.generativeai as genai
        
        genai.configure(api_key=self.api_key)
        model = genai.GenerativeModel('gemini-1.5-pro')
        
        # æ„å»ºè§’è‰²å‚è€ƒæ–‡æœ¬
        char_text = ""
        for char in character_refs:
            char_text += f"Character: {char['name']}\n"
        
        prompt = f"""
        You are analyzing a video segment. Below are character references and video frames.
        
        Character References:
        {char_text}
        
        Audio Transcript: {transcript}
        
        Please provide a detailed video scene description including:
        1. Characters' emotions and actions
        2. Motion dynamics and scene flow
        3. Visual elements and atmosphere
        4. Use character names if they appear in the video
        
        Respond in English only. Be descriptive and engaging.
        """
        
        # å‡†å¤‡å†…å®¹
        content = [prompt]
        for frame in frames:
            content.append(frame)
        
        try:
            response = model.generate_content(content)
            return response.text.strip()
        except Exception as e:
            print(f"Gemini API error: {e}")
            return "Video analysis failed"
    
    def _gemini_frame_selection(self, frames: List[Image.Image], description: str, 
                               duration: float) -> int:
        """ä½¿ç”¨Geminiè¿›è¡Œè§†é¢‘å¸§é€‰æ‹©"""
        import google.generativeai as genai
        
        genai.configure(api_key=self.api_key)
        model = genai.GenerativeModel('gemini-1.5-pro')
        
        prompt = f"""
        You are analyzing {len(frames)} consecutive video frames to find the best sequence.
        
        Scene Description: "{description}"
        Required Duration: {duration:.1f} seconds
        
        Requirements:
        1. Analyze ALL {len(frames)} frames
        2. Choose the best starting frame (0-{len(frames)-1})
        3. Maximum starting frame: {len(frames) - int(duration)}
        4. Return ONLY a single number - the frame index
        5. Select frames that best match the scene description
        
        Respond with just the frame number (0-{len(frames)-1}):
        """
        
        # å‡†å¤‡å†…å®¹
        content = [prompt]
        for frame in frames:
            content.append(frame)
        
        try:
            response = model.generate_content(content)
            
            # æå–æ•°å­—
            import re
            result = response.text.strip()
            numbers = re.findall(r'\d+', result)
            if numbers:
                frame_idx = int(numbers[0])
                return max(0, min(frame_idx, len(frames) - int(duration)))
            return 0
        except Exception as e:
            print(f"Gemini frame selection error: {e}")
            return 0


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šä½¿ç”¨GPT-4V
    processor = APIVideoProcessor(provider="openai", api_key="your-api-key")
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨Claude
    # processor = APIVideoProcessor(provider="claude", api_key="your-api-key")
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨Gemini
    # processor = APIVideoProcessor(provider="gemini", api_key="your-api-key")
    
    print("API Video Processor initialized successfully!")
